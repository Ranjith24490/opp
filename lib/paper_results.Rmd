---
title: A large-scale analysis of racial disparities in police stops across the United
  States
classoption: landscape
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
source(here::here("lib", "utils.R"))
library(tidyverse)
library(knitr)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
pfs <- read_rds(here::here("results", "prima_facie_stats.rds"))
vod <- read_rds(here::here("results", "veil_of_darkness.rds"))
disp <- read_rds(here::here("results", "disparity.rds"))
mj <- read_rds(here::here("results", "marijuana_legalization_analysis.rds"))
```


\section{Basic Numbers}


We have collected and released a dataset of 
`r comma_num(pfs$counts$collected$n_stops)` traffic stops carried out by 
`r pfs$counts$collected$n_states` (`r comma_num(pfs$counts$collected$n_stops_states)`)
state patrol agencies and
`r pfs$counts$collected$n_cities` (`r comma_num(pfs$counts$collected$n_stops_cities)`)
municipal police departments.

We analyzed a unique dataset detailing 
`r comma_num(pfs$counts$analyzed$n_stops)` traffic stops carried out by 
`r pfs$counts$analyzed$n_states` (`r comma_num(pfs$counts$analyzed$n_stops_states)`)
state patrol agencies and
`r pfs$counts$analyzed$n_cities` (`r comma_num(pfs$counts$analyzed$n_stops_cities)`)
municipal police departments.


\section{Prima Facie Stats}

Stop rates:
```{r stop_rates}
pfs$rates$stop %>%
  mutate(agency = if_else(city == "Statewide", "state patrol", "municipal pd")) %>% 
  group_by(agency, subject_race) %>%
  summarize(
    average_annual_stop_rate = mean(annual_stop_rate),
    n = n()
  ) %>%
  kable()
```

Search rates:
Number of state patrols with data for search analysis: 
`r pfs$rates$search %>% filter(city == "Statewide") %>% select(state) %>% n_distinct`

Number of city departments with data for search analysis: 
`r pfs$rates$search %>% filter(city != "Statewide") %>% select(state, city) %>% n_distinct`

```{r search_rates}
tbl <- pfs$rates$search %>% 
  mutate(agency = if_else(city == "Statewide", "state patrol", "municipal pd")) %>% 
  group_by(agency, subject_race) %>%
  summarize(
    average_n = mean(n),
    average_n_searches = mean(n_searches),
    average_search_rate = mean(search_rate),
    std_error = sqrt(
      average_search_rate * (1 - average_search_rate) / average_n
    ),
    CI_radius = 1.96 * std_error,
    CI_lower = average_search_rate - CI_radius,
    CI_upper = average_search_rate + CI_radius
  ) %>%
  ungroup()
tbl %>% kable()
ptbl <- tbl %>%
  mutate(
    n_success = average_n_searches,
    n_failure = average_n - average_n_searches
  )
pstate <- filter(ptbl, str_detect(agency, 'state'))
pcity <- filter(ptbl, str_detect(agency, 'municipal'))
```

Search p-values:
`r slice(pstate, 1:2) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pstate, 1:2) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pstate, -2) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pstate, -2) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pstate, 2:3) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pstate, 2:3) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pcity, 1:2) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pcity, 1:2) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pcity, -2) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pcity, -2) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pcity, 2:3) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pcity, 2:3) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`

\newpage

\section{Summary Table}

```{r search_rates}
pfs$summary_table
```

\newpage

\section{Veil of Darkness}

Number of stops in 7:00-7:15pm panel:
```{r}
vod$full$plots$states$TX$`7:00pm`$data$n %>% sum() %>% comma_num()
```

Number of stops in 7:15-7:30pm panel:
```{r}
vod$full$plots$states$TX$`7:15pm`$data$n %>% sum() %>% comma_num()
```

Number of stops in 7:30-7:45pm panel:
```{r}
vod$full$plots$states$TX$`7:30pm`$data$n %>% sum() %>% comma_num()
```

Total number of stops in 3 panels:
```{r}
comma_num(vod$full$plots$states$TX$`7:00pm`$data$n %>% sum() + vod$full$plots$states$TX$`7:15pm`$data$n %>% sum() +  vod$full$plots$states$TX$`7:30pm`$data$n %>% sum())
```

```{r veil_of_darkness-plot-trio, fig.height=4, fig.width=9}
d <- rbind(
  vod$full$plots$states$TX$`7:00pm`$data,
  vod$full$plots$states$TX$`7:15pm`$data,
  vod$full$plots$states$TX$`7:30pm`$data
) %>% 
  mutate(time_window = str_c(
    time_str, " to ", minute_to_time(rounded_minute + 15)
  ))

d %>% 
  ggplot(aes(minutes_since_dark, proportion_minority)) +
  geom_point(aes(size = n)) +
  geom_smooth(
    aes(y = avg_p_minority, color = is_dark),
    method = "lm", se = F, linetype = "dashed"
  ) +
  geom_vline(xintercept = -25, linetype = "dotted") +
  geom_vline(xintercept = 0, linetype = "dotted") +
  geom_ribbon(
    data = filter(d, is_dark),
    aes(ymin = avg_p_minority - se, ymax = avg_p_minority + se),
    alpha=0.3
  ) +
  geom_ribbon(
    data = filter(d, !is_dark),
    aes(ymin = avg_p_minority - se, ymax = avg_p_minority + se),
    alpha=0.3
  ) +
  scale_x_continuous(
    "Minutes since dusk",
    limits = c(-90, 60),
    breaks = seq(-90, 60, 30)
  ) +
  scale_y_continuous(
    str_c("Percent of stopped drivers who are black"),
    limits = c(0.15, 0.35),
    breaks = seq(0.0, 1.0, 0.05), 
    labels = scales::percent_format(accuracy = 1)
  ) +
  scale_color_manual(values = c("blue", "blue")) +
  theme_bw(base_size = 16) +
  theme(
    legend.position = 'none',
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  facet_grid(cols = vars(time_str))
```

\subsection{Main model results}

```{r veil_of_darkness-main_mod}
vod$dst %>% 
  select(-data, -base_controls) %>% 
  filter(spline_degree == 6, agency_control, !interact_time_loc) %>% 
  kable()
```

Point estimates: `r vod$dst %>% filter(spline_degree == 6, agency_control, !interact_time_loc) %>% pull(is_dark)`

95% CI (lower): `r vod$dst %>% filter(spline_degree == 6, agency_control, !interact_time_loc) %>% pull(is_dark) - 1.96*(vod$dst %>% filter(spline_degree == 6, agency_control, !interact_time_loc) %>% pull(std_error))`

95% CI (upper): `r vod$dst %>% filter(spline_degree == 6, agency_control, !interact_time_loc) %>% pull(is_dark) + 1.96*(vod$dst %>% filter(spline_degree == 6, agency_control, !interact_time_loc) %>% pull(std_error))`

\newpage
\subsection{Model results for the appendix/footnote reference}

Number of robustness check models with results not statistically significant: 
`r vod$dst %>% filter(is_dark + 1.96*std_error >= 0) %>% nrow`

```{r veil_of_darkness-appendix_mods}
bind_rows(
  vod$dst %>%
    select(-data, -base_controls) %>% 
    mutate(model = "DST"),
  vod$full$coefficients %>%
    select(-data, -base_controls) %>% 
    mutate(model = "Full year")
) %>%
  arrange(model, agency_control, interact_time_loc, spline_degree) %>% 
  select(model, agency, is_dark, std_error, p_value, everything()) %>%
  kable()
```

\newpage

\section{Search Disparity Analysis}

State patrol agencies analyzed: 
`r disp$state$outcome$results$hit_rates$geography %>% unique()`

Total number of state patrol searches: 
`r disp$state$outcome$results$hit_rates$n_search_conducted %>% sum() %>% comma_num()`

Municipal departments analyzed:
`r disp$city$outcome$results$hit_rates$geography %>% unique()`

Total number of municipal pd searches: 
`r disp$city$outcome$results$hit_rates$n_search_conducted %>% sum() %>% comma_num()`

\subsection{Outcome Test}

City and State outcomes:
```{r disparity-hit_rates-tables}
tbl <- rbind(
  disp$city$outcome$results$aggregate_hit_rates %>% 
    mutate(
      geography = "cities",
      std_error = sqrt(average_hit_rate * (1 - average_hit_rate) / average_n_search_conducted),
      lower_CI = average_hit_rate - 1.96 * std_error,
      upper_CI = average_hit_rate + 1.96 * std_error,
    ),
  disp$state$outcome$results$aggregate_average_hit_rates %>% 
    mutate(
      geography = "states",
      std_error = sqrt(average_hit_rate * (1 - average_hit_rate) / average_n_search_conducted),
      lower_CI = average_hit_rate - 1.96 * std_error,
      upper_CI = average_hit_rate + 1.96 * std_error
    )
  ) %>% 
  select(geography, everything())
tbl %>% kable()
ptbl <- tbl %>%
  mutate(
    n_success = average_n_contraband_found,
    n_failure = average_n_search_conducted - average_n_contraband_found
  )
pstate <- filter(ptbl, geography == "states")
pcity <- filter(ptbl, geography == "cities")
```

Hit rate p-values:
`r slice(pstate, 1:2) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pstate, 1:2) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pstate, -2) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pstate, -2) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pstate, 2:3) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pstate, 2:3) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pcity, 1:2) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pcity, 1:2) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pcity, -2) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pcity, -2) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`
`r slice(pcity, 2:3) %>% select(agency, subject_race) %>% kable()`
p-value: `r slice(pcity, 2:3) %>% select(n_success, n_failure) %>% as.matrix %>% prop.test %$% p.value`

\newpage

```{r disparity-city-hit_rates-plot, fig.height=6, fig.width=10}
axis_min <- max(
  min(
    disp$city$outcome$plots$aggregate$data$minority_rate,
    disp$city$outcome$plots$aggregate$data$majority_rate
  ) - 0.05, 0
)
axis_max <- min(
  max(
    disp$city$outcome$plots$aggregate$data$minority_rate,
    disp$city$outcome$plots$aggregate$data$majority_rate
  ) + 0.05, 1
)
disp$city$outcome$plots$aggregate + 
  theme(legend.position = "none", plot.title = element_blank()) + 
  scale_x_continuous(
    limits = c(axis_min, axis_max),
    labels = scales::percent_format(accuracy = 1), expand = c(0,0)
  ) + 
  scale_y_continuous(
    limits = c(axis_min, axis_max),
    labels = scales::percent_format(accuracy = 1),expand = c(0,0)
  ) 
```

\newpage

```{r disparity-state-hit_rates-plot, fig.height=6, fig.width=10}
disp$state$outcome$plots$aggregate + theme(legend.position = "none")
```

\newpage

\subsection{Threshold test results}

Scaling priors by 0.5x:
```{r disparity-thresholds-tables-0.5}
kable(disp$city$threshold$prior_scaling_factor_0.5$results$aggregate_thresholds)
kable(disp$state$threshold$prior_scaling_factor_0.5$results$aggregate_thresholds)
```

Priors at 1.0 (normal):
```{r disparity-thresholds-tables-1.0}
kable(disp$city$threshold$prior_scaling_factor_1$results$aggregate_thresholds)
kable(disp$state$threshold$prior_scaling_factor_1$results$aggregate_thresholds)
```

Scaling priors by 1.5x:
```{r disparity-thresholds-tables-1.5}
kable(disp$city$threshold$prior_scaling_factor_1.5$results$aggregate_thresholds)
kable(disp$state$threshold$prior_scaling_factor_1.5$results$aggregate_thresholds)
```

\newpage
Threshold plot for normal priors:

```{r disparity-city-thresholds-plot, fig.height=5, fig.width=10}
axis_min <- max(
  min(
    disp$city$threshold$prior_scaling_factor_1$plots$aggregate$data$minority_rate,
    disp$city$threshold$prior_scaling_factor_1$plots$aggregate$data$majority_rate
  ) - 0.05, 0
)
axis_max <- min(
  max(
    disp$city$threshold$prior_scaling_factor_1$plots$aggregate$data$minority_rate,
    disp$city$threshold$prior_scaling_factor_1$plots$aggregate$data$majority_rate
  ) + 0.05, 1
)
disp$city$threshold$prior_scaling_factor_1$plots$aggregate + 
  theme(legend.position = "none", plot.title = element_blank()) + 
  scale_x_continuous(
    limits = c(axis_min, axis_max), breaks = seq(0, .5, 0.05),
    labels = scales::percent_format(accuracy = 1), expand = c(0,0)
  ) + 
  scale_y_continuous(
    limits = c(axis_min, axis_max), breaks = seq(0, .5, 0.05),
    labels = scales::percent_format(accuracy = 1),expand = c(0,0)
  ) 
```

\newpage
Normal priors:

```{r disparity-state-thresholds-plot, fig.height=5, fig.width=10}
disp$state$threshold$prior_scaling_factor_1$plots$aggregate + theme(legend.position = "none")
```

\newpage
Normal priors:

```{r disparity-city-ppc-search, fig.height=5, fig.width=8}
disp$city$threshold$prior_scaling_factor_1$ppc$search_rate
```

\newpage
Normal priors:

```{r disparity-city-ppc-hit, fig.height=5, fig.width=8}
disp$city$threshold$prior_scaling_factor_1$ppc$hit_rate
```

\newpage
Normal priors:

```{r disparity-state-ppc-search, fig.height=5, fig.width=8}
disp$state$threshold$prior_scaling_factor_1$ppc$search_rate
```

\newpage
Normal priors:

```{r disparity-state-ppc-hit, fig.height=5, fig.width=8}
disp$state$threshold$prior_scaling_factor_1$ppc$hit_rate
```

\newpage


\section{Marijuana Legalization Analysis}


```{r mj_table}
kable(mj$tables$search_rate_difference_in_difference_coefficients)
```

\newpage

```{r mj_treatment_search_rates_plot, fig.height=5, fig.width=10}
mj$plots$treatment_search_rates + theme(
  legend.position = c(0.88, 0.80),
  axis.title.y = element_text(size = 12)
)
```
```{r mj_treatment_search_rates_counts, fig.height=5, fig.width=10}
mj$plots$treatment_search_rates$counts 
```

\newpage

```{r mj_treatment_misdemeanor_rates_plot, fig.height=5, fig.width=10}
mj$plots$treatment_misdemeanor_rates$plot + theme(
  legend.position = c(0.88, 0.80),
  axis.title.y = element_text(size = 12)
)
```

```{r mj_treatment_misdemeanor_rates_counts, fig.height=5, fig.width=10}
mj$plots$treatment_misdemeanor_rates$counts 
```

\newpage

```{r mj_inferred_threshold_plot, fig.height=5, fig.width=10}
mj$plots$inferred_threshold_changes$prior_scaling_factor_1$plot
```

Prior robustness checks

Scaling factor 0.5:

WA rhat = `r mj$plots$inferred_threshold_changes$prior_scaling_factor_0.5$metadata$wa_rhat`

WA n_eff = `r mj$plots$inferred_threshold_changes$prior_scaling_factor_0.5$metadata$wa_n_eff`

CO rhat = `r mj$plots$inferred_threshold_changes$prior_scaling_factor_0.5$metadata$co_rhat`

CO n_eff = `r mj$plots$inferred_threshold_changes$prior_scaling_factor_0.5$metadata$co_n_eff`

```{r mj_inferred_threshold_plot_0.5_prior_scale, fig.height=5, fig.width=10}
mj$plots$inferred_threshold_changes$prior_scaling_factor_0.5$plot
```

Scaling factor 1.5:

WA rhat = `r mj$plots$inferred_threshold_changes$prior_scaling_factor_1.5$metadata$wa_rhat`

WA n_eff = `r mj$plots$inferred_threshold_changes$prior_scaling_factor_1.5$metadata$wa_n_eff`

CO rhat = `r mj$plots$inferred_threshold_changes$prior_scaling_factor_1.5$metadata$co_rhat`

CO n_eff = `r mj$plots$inferred_threshold_changes$prior_scaling_factor_1.5$metadata$co_n_eff`

```{r mj_inferred_threshold_plot_1.5_prior_scale, fig.height=5, fig.width=10}
mj$plots$inferred_threshold_changes$prior_scaling_factor_1.5$plot
```

\newpage

```{r mj_control_search_rates_plot, fig.height=5, fig.width=10}
mj$plots$control_search_rates$plot
```

```{r mj_control_search_rates_counts, fig.height=5, fig.width=10}
mj$plots$control_search_rates$counts
```
